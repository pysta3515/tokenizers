728:PreTokenizedString { original: "hello, ðŸ¤— [PAD] [UNK]",
    splits: [
        Split { normalized: NormalizedString { original: "hello, ðŸ¤— ", normalized: "hello, ðŸ¤— ", alignments: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 11), (7, 11), (7, 11), (7, 11), (11, 12)], original_shift: 0 }, tokens: None }, 
        Split { normalized: NormalizedString { original: "[PAD]", normalized: "[PAD]", alignments: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)], original_shift: 12 }, tokens: Some([Token { id: 3, value: "[PAD]", offsets: (0, 5) }]) }, 
        Split { normalized: NormalizedString { original: " ", normalized: " ", alignments: [(0, 1)], original_shift: 17 }, tokens: None }, 
        Split { normalized: NormalizedString { original: "[UNK]", normalized: "[UNK]", alignments: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)], original_shift: 18 }, tokens: Some([Token { id: 0, value: "[UNK]", offsets: (0, 5) }]) }] }
730:PreTokenizedString { original: "hello, ðŸ¤— [PAD] [UNK]",
    splits: [
        Split { normalized: NormalizedString { original: "hello", normalized: "hello", alignments: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)], original_shift: 0 }, tokens: None }, 
        Split { normalized: NormalizedString { original: ",", normalized: ",", alignments: [(0, 1)], original_shift: 5 }, tokens: None }, 
        Split { normalized: NormalizedString { original: "ðŸ¤—", normalized: "ðŸ¤—", alignments: [(0, 4), (0, 4), (0, 4), (0, 4)], original_shift: 7 }, tokens: None }, 
        Split { normalized: NormalizedString { original: "[PAD]", normalized: "[PAD]", alignments: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)], original_shift: 12 }, tokens: Some([Token { id: 3, value: "[PAD]", offsets: (0, 5) }]) }, 
        Split { normalized: NormalizedString { original: "[UNK]", normalized: "[UNK]", alignments: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)], original_shift: 18 }, tokens: Some([Token { id: 0, value: "[UNK]", offsets: (0, 5) }]) }] }
741:Encoding { 
    ids: [817, 409, 16, 0, 3, 0], 
    type_ids: [0, 0, 0, 0, 0, 0], 
    tokens: ["hel", "lo", ",", "[UNK]", "[PAD]", "[UNK]"], 
    words: [Some(0), Some(0), Some(1), Some(2), Some(3), Some(4)], 
    offsets: [(0, 3), (3, 5), (5, 6), (7, 11), (12, 17), (18, 23)], 
    special_tokens_mask: [0, 0, 0, 0, 0, 0], 
    attention_mask: [1, 1, 1, 1, 1, 1], 
    overflowing: [], 
    sequence_ranges: {} }
839:["hel", "lo", ",", "[UNK]", "[PAD]", "[UNK]"]
out:["hel", "lo", ",", "[UNK]", "[PAD]", "[UNK]"]